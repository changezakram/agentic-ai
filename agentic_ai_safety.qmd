---
author: Changez Akram
categories:
- Agentic AI
- AI Security
- Responsible AI
date: today
execute:
  echo: false
  message: false
  warning: false
format:
  html:
    code-fold: false
    number-sections: true
    theme: cosmo
    toc: true
    toc-depth: 3
title: Towards Building Safe and Secure Agentic AI
---

## 1. Introduction --- The Year of Agents

2025 is increasingly being described as the *"Year of Agents."* Frontier
AI has moved beyond static text generation into a new class of systems
that can observe, reason, plan, and act. We are now seeing rapid
progress in web-browsing agents, software-operating agents, coding
agents, and even robotics. These systems no longer sit passively behind
a chat interface. They interact directly with real systems, APIs, files,
networks, users, and in some cases, physical environments.

As impressive as this progress is, it introduces a new reality: **AI
risk now scales directly with AI capability.** As agents gain autonomy,
speed, and access to critical systems, small errors can propagate into
large failures.

The *International AI Safety Report* led by Yoshua Bengio highlights a
broad spectrum of emerging risks, ranging from misuse and manipulation
to systemic failures and unintended real-world consequences. These risks
are no longer theoretical. As agentic systems begin to act independently
across digital and physical domains, the potential blast radius of
failure expands dramatically.

Crucially, we must now evaluate AI not in controlled laboratory
conditions, but **in the presence of intelligent adversaries**.

History shows that attackers always follow, and often anticipate, major
technological shifts. Every platform breakthrough---networks, cloud
computing, mobile devices, and cryptocurrencies---has been matched by a
rapid evolution in attack methods. With AI, the stakes rise sharply.
Agents increasingly control workflows, decisions, and resources at
machine speed, often with limited human oversight. This fundamentally
changes the security calculus. The incentives to compromise AI systems
grow, and the downstream impact of misuse becomes far more severe.

This brings us to an often misunderstood distinction: the difference
between **AI safety** and **AI security**.

AI safety focuses on preventing the system from causing harm to the
external world. This includes alignment with human intent, avoidance of
toxic or biased outputs, reduction of hallucinations, and constraints on
unsafe autonomous behavior. AI security, by contrast, focuses on
protecting the system itself from malicious external actors, including
attacks such as prompt injection, tool and API abuse, data poisoning,
model extraction, and adversarial manipulation.

In the age of agents, these two domains are inseparable.

An aligned system that is insecure can be turned into a high-impact
weapon.\
A secure system that is misaligned can still cause harm at scale.

**AI safety must now function inside an adversarial environment.**
Alignment mechanisms must remain robust not only under normal
conditions, but under continuous attack. This marks a fundamental shift
in how we must think about deploying autonomous systems.

The central premise of secure agentic AI is therefore simple but
profound:

> We are no longer securing models. We are securing autonomous
> decision-making systems.

## 2. Agentic AI --- From Models to Systems

Most AI applications in production today are still **simple LLM
applications**. They are passive responders. Agentic AI transforms them
into **autonomous actors** operating across real workflows.

Agents embed probabilistic neural reasoning inside deterministic
software stacks, creating hybrid systems that perceive, plan, and act.
This hybrid structure is precisely what gives agents their power---and
what makes them fundamentally harder to secure.

## 3. The Agent Threat Landscape

Agents ingest untrusted inputs, retrieve live data, invoke probabilistic
reasoning, and execute real-world actions. There is no single "front
door" to defend. **Every interface becomes an attack surface.**

Attacks now include SQL injection through generated queries, remote code
execution through auto-generated scripts, indirect prompt injection
through poisoned documents, and backdoors embedded in retrieval systems.

Agents collapse the boundary between **language, logic, and execution**.
Semantic manipulation becomes as powerful as technical exploitation.

## 4. Evaluation and Risk Assessment

Evaluating the model alone is insufficient. We must evaluate the
**end-to-end agentic system**.

AgentXploit demonstrates black-box red teaming where attackers
manipulate only the external environment. Using fuzzing and Monte Carlo
Tree Search, hidden failure modes emerge through multi-step reasoning.

In one scenario, a shopping agent reads a malicious product review that
instructs it to visit a malware site---without any direct prompt
injection.

## 5. Defenses in Agentic AI

Security must be layered. No single control is sufficient.

Model hardening reduces risk but is not a security boundary. Input
sanitization and guardrails filter untrusted data before reasoning.
Policy enforcement at the tool layer ensures that even if a model is
persuaded, **malicious actions are blocked at execution time**.
Privilege separation limits blast radius. Monitoring assumes failure
will occur.

## Conclusion --- Securing the Age of Autonomous Systems

Agentic AI introduces complex hybrid systems that dramatically expand
the attack surface. We must move beyond model evaluation to full-system
adversarial testing and layered defenses.

The next phase of AI will not be defined by intelligence alone.\
It will be defined by **whether we can trust the systems we empower to
act on our behalf**.
